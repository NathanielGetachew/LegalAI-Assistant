{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "LdHK9LXYdObb",
      "metadata": {
        "id": "LdHK9LXYdObb"
      },
      "source": [
        "Use Case: Build a Legal AI Assistant that classifies legal complaint texts by potential violation type (e.g., labor, human rights, environmental), and explains the classification using natural language reasoning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a0a4ded",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "0a0a4ded",
        "outputId": "92c659fb-1911-494b-a941-86515a7a27f8",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am a large language model, trained by Google.  I'm an AI designed to process information and respond to a wide range of prompts and questions.  Essentially, I'm a computer program that can communicate and generate human-like text.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Install the Gemini SDK\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "# STEP 2: Load  Gemini API Key\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Replace YOUR_API_KEY with your actual Gemini API key\n",
        "GEMINI_API_KEY = \"AIzaSyC5LBQFarxRwaadGBKywwig0UGJLpFOMFY\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# STEP 3: Initialize the Gemini Pro model\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# STEP 4: Test the model\n",
        "response = model.generate_content(\"Hello, what are you?\")\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "xBoqeUvZ4GRU",
      "metadata": {
        "id": "xBoqeUvZ4GRU"
      },
      "outputs": [],
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        \"id\": \"001\",\n",
        "        \"case_text\": \"An employee was hired for a 2-year contract but was terminated without cause after 6 months.\",\n",
        "        \"context\": \"The contract stipulated early termination requires a 3-month severance.\",\n",
        "        \"expected\": \"Breach of employment contract\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"002\",\n",
        "        \"case_text\": \"A blogger accused a public figure of financial fraud without presenting any evidence.\",\n",
        "        \"context\": \"The public figure suffered reputational damage and financial losses.\",\n",
        "        \"expected\": \"Defamation (libel)\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"003\",\n",
        "        \"case_text\": \"A tech company copied key features from a patented software system without licensing.\",\n",
        "        \"context\": \"The original company filed a lawsuit under U.S. patent law.\",\n",
        "        \"expected\": \"Patent infringement\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"004\",\n",
        "        \"case_text\": \"A customer slipped on a wet floor in a grocery store. There was no warning sign.\",\n",
        "        \"context\": \"The customer suffered a broken arm and sued for negligence.\",\n",
        "        \"expected\": \"Premises liability due to negligence\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"005\",\n",
        "        \"case_text\": \"A seller misrepresented the condition of a used car as 'like new,' hiding previous accident damage.\",\n",
        "        \"context\": \"The buyer discovered the issue after purchase and sued for damages.\",\n",
        "        \"expected\": \"Fraudulent misrepresentation\"\n",
        "    },\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "l2umYbMXeta2",
      "metadata": {
        "id": "l2umYbMXeta2"
      },
      "outputs": [],
      "source": [
        "def generate_cot_prompt(case_text, context):\n",
        "    return f\"\"\"\n",
        "You are a legal expert tasked with analyzing a legal document and identifying any potential violations.\n",
        "\n",
        "Case: {case_text}\n",
        "Context: {context}\n",
        "\n",
        "**Instructions**:\n",
        "- First, classify the violation (e.g., breach of contract, personal injury, etc.).\n",
        "- Then, explain step-by-step why you think this is the violation.\n",
        "- Provide relevant legal clauses or precedents to support your reasoning.\n",
        "\n",
        "Please answer the following questions:\n",
        "1. What violation has occurred in this case?\n",
        "2. Why do you classify this as [violation type]?\n",
        "3. What are the relevant legal clauses or precedents that apply to this violation?\n",
        "\n",
        "**Reasoning (Chain-of-Thought)**:\n",
        "1. Start by analyzing the case details step by step.\n",
        "2. Provide intermediate conclusions.\n",
        "3. Conclude with the final classification and reasoning.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cHnVDGhIhWSo",
      "metadata": {
        "id": "cHnVDGhIhWSo"
      },
      "outputs": [],
      "source": [
        "def classify_violation_with_cot(case_text, context):\n",
        "    \"\"\"\n",
        "    Classify a legal violation and provide reasoning using Chain-of-Thought.\n",
        "    \"\"\"\n",
        "    # Generate the prompt using CoT\n",
        "    cot_prompt = generate_cot_prompt(case_text, context)\n",
        "\n",
        "    # Use Gemini to generate the content\n",
        "    response = model.generate_content(cot_prompt)\n",
        "\n",
        "    # Extract the text from the response\n",
        "    classification_and_reasoning = response.text\n",
        "    return classification_and_reasoning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "EYQkPmafhWPH",
      "metadata": {
        "id": "EYQkPmafhWPH"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "import google.generativeai as genai\n",
        "\n",
        "def test_legal_scenarios(test_cases, model):\n",
        "    markdown_table = \"| Case ID | Expected | Model Output | Match | Notes |\\n\"\n",
        "    markdown_table += \"|---------|----------|--------------|-------|-------|\\n\"\n",
        "\n",
        "    for case in test_cases:\n",
        "        prompt = generate_cot_prompt(case[\"case_text\"], case[\"context\"])\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            output = response.text.strip()\n",
        "            first_line = output.split('\\n')[0]\n",
        "            match = case[\"expected\"].lower() in output.lower()\n",
        "            markdown_table += f\"| {case['id']} | {case['expected']} | {first_line} | {'✅' if match else '❌'} | {'-' if match else 'Check classification'} |\\n\"\n",
        "        except Exception as e:\n",
        "            markdown_table += f\"| {case['id']} | {case['expected']} | ERROR | ❌ | {str(e)[:40]}... |\\n\"\n",
        "\n",
        "    display(Markdown(markdown_table))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cCyJ0UK54aVK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cCyJ0UK54aVK",
        "outputId": "7b7f0290-841d-4e8a-b34c-8c1ece420962"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "| Case ID | Expected | Model Output | Match | Notes |\n",
              "|---------|----------|--------------|-------|-------|\n",
              "| 001 | Breach of employment contract | **Reasoning (Chain-of-Thought):** | ✅ | - |\n",
              "| 002 | Defamation (libel) | **Reasoning (Chain-of-Thought):** | ✅ | - |\n",
              "| 003 | Patent infringement | ERROR | ❌ | Invalid operation: The `response.text` q... |\n",
              "| 004 | Premises liability due to negligence | **1. What violation has occurred in this case?** | ❌ | Check classification |\n",
              "| 005 | Fraudulent misrepresentation | **Chain-of-Thought:** | ✅ | - |\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_legal_scenarios(test_cases, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vplIrrIChhO6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "vplIrrIChhO6",
        "outputId": "953ddab1-3bb8-483b-bf45-3d67375b5f6a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pUDJlBiCkNng",
      "metadata": {
        "id": "pUDJlBiCkNng"
      },
      "outputs": [],
      "source": [
        "def build_constrained_prompt(scenario_text, labels):\n",
        "    # Join the list of labels into a comma-separated string, with each label wrapped in double quotes\n",
        "    label_list = ', '.join([f'\"{label}\"' for label in labels])\n",
        "    \n",
        "    # Return the complete prompt\n",
        "    return f\"\"\"\n",
        "Given the legal scenario below, classify it into one of the following categories:\n",
        "{label_list}\n",
        "\n",
        "Scenario: \"{scenario_text}\"\n",
        "\n",
        "Only return the classification label. Do not explain.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c848f556",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def classify_with_self_consistency(case_text, model, label_options, num_responses=5):\n",
        "    # Create the prompt based on the case text and possible labels\n",
        "    prompt = build_constrained_prompt(case_text, label_options)\n",
        "    \n",
        "    # Collect responses from the model\n",
        "    responses = [model.generate_content(prompt).text.strip().lower() for _ in range(num_responses)]\n",
        "\n",
        "    # Filter out responses that are not in the predefined labels\n",
        "    responses = [resp for resp in responses if resp in [label.lower() for label in label_options]]\n",
        "    \n",
        "    # If no valid responses, return \"unclear\"\n",
        "    if not responses:\n",
        "        return \"unclear\"\n",
        "    \n",
        "    # Return the most frequent response using majority voting\n",
        "    final_prediction = Counter(responses).most_common(1)[0][0]\n",
        "    return final_prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dfa15cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def evaluate_model_with_self_consistency(test_cases, model, label_options, num_responses=5):\n",
        "    y_true = []  # List to store the true labels (expected results)\n",
        "    y_pred = []  # List to store the predicted labels (model outputs)\n",
        "    \n",
        "    markdown_table = \"| Case ID | Expected | Predicted | Match |\\n|---------|----------|-----------|-------|\"\n",
        "    \n",
        "    # Iterate over each test case\n",
        "    for case in test_cases:\n",
        "        expected = case['expected'].strip().lower()  # True label (in lowercase)\n",
        "        \n",
        "        # Get the prediction from the model using self-consistency\n",
        "        prediction = classify_with_self_consistency(case['input'], model, label_options, num_responses)\n",
        "        \n",
        "        y_true.append(expected)  # Append the expected result\n",
        "        y_pred.append(prediction)  # Append the predicted result\n",
        "\n",
        "        # Check if the prediction matches the expected result\n",
        "        match = \"✅\" if prediction == expected else \"❌\"\n",
        "        \n",
        "        # Append the results to the markdown table\n",
        "        markdown_table += f\"\\n| {case['id']} | {expected} | {prediction} | {match} |\"\n",
        "    \n",
        "    # Display the markdown table with results\n",
        "    display(Markdown(markdown_table))\n",
        "    \n",
        "    # Calculate and print the classification report (precision, recall, F1-score)\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed89bed8",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_options = [\n",
        "    \"breach of employment contract\",\n",
        "    \"defamation (libel)\",\n",
        "    \"patent infringement\",\n",
        "    \"premises liability due to negligence\",\n",
        "    \"fraudulent misrepresentation\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f0b6f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_model_with_self_consistency(test_cases, model, label_options, num_responses=5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
